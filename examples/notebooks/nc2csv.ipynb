{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac284b-46a8-47e2-9964-2809d0e87438",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xarray\n",
    "!pip install netCDF4\n",
    "!pip install h5netcdf\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364248b-2814-4615-9663-32217bba1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # For nice progress bars in Jupyter\n",
    "import pandas as pd\n",
    "\n",
    "def nc_to_csv_xarray(nc_file, output_dir=None, engine='netcdf4'):\n",
    "    \"\"\"\n",
    "    Convert a NetCDF file to CSV using xarray\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nc_file : str\n",
    "        Path to the NetCDF file\n",
    "    output_dir : str, optional\n",
    "        Directory to save CSV files\n",
    "    engine : str, optional\n",
    "        Engine to use for opening the NetCDF file. Options: 'netcdf4', 'scipy', 'h5netcdf'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if conversion was successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Create output directory if specified\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(nc_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get base filename without extension\n",
    "    base_name = os.path.splitext(os.path.basename(nc_file))[0]\n",
    "    \n",
    "    try:\n",
    "        # Open the NetCDF file with xarray, explicitly specifying the engine\n",
    "        print(f\"Opening file with {engine} engine: {nc_file}\")\n",
    "        ds = xr.open_dataset(nc_file)\n",
    "        \n",
    "        # Print dataset information\n",
    "        print(f\"NetCDF file information:\")\n",
    "        print(f\"Dimensions: {list(ds.dims.keys())}\")\n",
    "        print(f\"Variables: {list(ds.variables.keys())}\")\n",
    "        print(f\"Data variables: {list(ds.data_vars.keys())}\")\n",
    "        print(f\"Coordinates: {list(ds.coords.keys())}\")\n",
    "        \n",
    "        # Export each data variable to CSV\n",
    "        for var_name in ds.data_vars:\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            var = ds[var_name]\n",
    "            print(f\"Dimensions: {var.dims}\")\n",
    "            print(f\"Shape: {var.shape}\")\n",
    "            \n",
    "            try:\n",
    "                # Convert to dataframe\n",
    "                df = var.to_dataframe()\n",
    "                \n",
    "                # Reset index to convert multi-index to columns (makes CSV more readable)\n",
    "                df = df.reset_index()\n",
    "                \n",
    "                # Save to CSV\n",
    "                csv_filename = f\"{base_name}_{var_name}.csv\"\n",
    "                csv_path = os.path.join(output_dir, csv_filename)\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"Saved {csv_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing variable {var_name}: {e}\")\n",
    "        \n",
    "        # Option to export the entire dataset to a single CSV\n",
    "        try:\n",
    "            print(\"\\nAttempting to export full dataset to single CSV...\")\n",
    "            df_all = ds.to_dataframe().reset_index()\n",
    "            all_csv_path = os.path.join(output_dir, f\"{base_name}_all_variables.csv\")\n",
    "            df_all.to_csv(all_csv_path, index=False)\n",
    "            print(f\"Saved complete dataset to {all_csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not export full dataset to single CSV: {e}\")\n",
    "        \n",
    "        ds.close()\n",
    "        print(f\"\\nConversion complete for {nc_file}!\")\n",
    "        return True\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error opening the file with {engine} engine: {e}\")\n",
    "        \n",
    "        # Try alternative engines if the specified one fails\n",
    "        alternative_engines = ['netcdf4', 'scipy', 'h5netcdf']\n",
    "        if engine in alternative_engines:\n",
    "            alternative_engines.remove(engine)\n",
    "        \n",
    "        for alt_engine in alternative_engines:\n",
    "            try:\n",
    "                print(f\"Attempting to open with {alt_engine} engine...\")\n",
    "                ds = xr.open_dataset(nc_file, engine=alt_engine)\n",
    "                ds.close()\n",
    "                # If successful, recursively call this function with the working engine\n",
    "                return nc_to_csv_xarray(nc_file, output_dir, alt_engine)\n",
    "            except Exception as e2:\n",
    "                print(f\"Failed with {alt_engine} engine: {e2}\")\n",
    "        \n",
    "        print(f\"\\nCould not open the NetCDF file {nc_file} with any available engine.\")\n",
    "        return False\n",
    "\n",
    "def process_directory(input_dir, output_dir=None, engine='netcdf4', file_patterns=None, \n",
    "                     recursive=False, max_depth=None, preserve_structure=True):\n",
    "    \"\"\"\n",
    "    Process all files matching patterns in a directory\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory containing files to process\n",
    "    output_dir : str, optional\n",
    "        Directory to save CSV files. If None, will create a 'csv_output' subfolder\n",
    "    engine : str, optional\n",
    "        Engine to use for opening the files\n",
    "    file_patterns : list, optional\n",
    "        List of file patterns to match (default: [\"*.nc\"])\n",
    "    recursive : bool, optional\n",
    "        Whether to search for files recursively in subdirectories\n",
    "    max_depth : int, optional\n",
    "        Maximum recursion depth (None for no limit)\n",
    "    preserve_structure : bool, optional\n",
    "        Whether to preserve the directory structure in the output\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with statistics about the conversion process\n",
    "    \"\"\"\n",
    "    # Set default file patterns if none provided\n",
    "    if file_patterns is None:\n",
    "        file_patterns = [\"*.nc\"]\n",
    "    elif isinstance(file_patterns, str):\n",
    "        file_patterns = [file_patterns]\n",
    "        \n",
    "    # Normalize paths\n",
    "    input_dir = os.path.abspath(input_dir)\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.join(input_dir, 'csv_output')\n",
    "    else:\n",
    "        output_dir = os.path.abspath(output_dir)\n",
    "    \n",
    "    # Make sure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all matching files in the directory\n",
    "    all_files = []\n",
    "    \n",
    "    for pattern in file_patterns:\n",
    "        if recursive:\n",
    "            if max_depth is None:\n",
    "                # No depth limit, use rglob\n",
    "                matching_files = [str(p) for p in Path(input_dir).rglob(pattern)]\n",
    "            else:\n",
    "                # Limited depth recursion\n",
    "                matching_files = []\n",
    "                for depth in range(max_depth + 1):\n",
    "                    parts = ['*'] * depth\n",
    "                    if parts:\n",
    "                        search_pattern = os.path.join(input_dir, *parts, pattern)\n",
    "                    else:\n",
    "                        search_pattern = os.path.join(input_dir, pattern)\n",
    "                    matching_files.extend(glob.glob(search_pattern))\n",
    "        else:\n",
    "            # Non-recursive, just use glob\n",
    "            matching_files = glob.glob(os.path.join(input_dir, pattern))\n",
    "        \n",
    "        all_files.extend(matching_files)\n",
    "    \n",
    "    # Remove duplicates (in case patterns overlap)\n",
    "    all_files = list(set(all_files))\n",
    "    \n",
    "    if not all_files:\n",
    "        patterns_str = \", \".join(file_patterns)\n",
    "        print(f\"No matching files found in {input_dir} with patterns: {patterns_str}\")\n",
    "        return {\"total\": 0, \"successful\": 0, \"failed\": 0, \"time\": 0}\n",
    "    \n",
    "    print(f\"Found {len(all_files)} files to process in {input_dir}\")\n",
    "    \n",
    "    # Process each file\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, file_path in enumerate(all_files, 1):\n",
    "        print(f\"\\n[{i}/{len(all_files)}] Processing: {os.path.relpath(file_path, input_dir)}\")\n",
    "        \n",
    "        # Determine the output directory\n",
    "        if preserve_structure:\n",
    "            # Keep the same directory structure in the output\n",
    "            rel_path = os.path.relpath(os.path.dirname(file_path), input_dir)\n",
    "            file_output_dir = os.path.join(output_dir, rel_path, os.path.splitext(os.path.basename(file_path))[0])\n",
    "        else:\n",
    "            # Flatten the structure\n",
    "            file_output_dir = os.path.join(output_dir, os.path.splitext(os.path.basename(file_path))[0])\n",
    "        \n",
    "        if nc_to_csv_xarray(file_path, file_output_dir, engine):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Conversion Summary for {input_dir}:\")\n",
    "    print(f\"Total files processed: {len(all_files)}\")\n",
    "    print(f\"Successfully converted: {successful}\")\n",
    "    print(f\"Failed to convert: {failed}\")\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return {\n",
    "        \"total\": len(all_files),\n",
    "        \"successful\": successful,\n",
    "        \"failed\": failed,\n",
    "        \"time\": elapsed_time\n",
    "    }\n",
    "\n",
    "def process_specific_files(file_paths, output_base_dir=None, engine='netcdf4', preserve_structure=True):\n",
    "    \"\"\"\n",
    "    Process a list of specific files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_paths : str or list of str\n",
    "        Comma-separated string or list of specific files to process\n",
    "    output_base_dir : str, optional\n",
    "        Base directory for all outputs\n",
    "    engine : str, optional\n",
    "        Engine to use for opening the files\n",
    "    preserve_structure : bool, optional\n",
    "        Whether to preserve the directory structure in the output\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with statistics about the conversion process\n",
    "    \"\"\"\n",
    "    # Process file paths (accept both comma-separated string and list)\n",
    "    if isinstance(file_paths, str):\n",
    "        file_paths = [f.strip() for f in file_paths.split(',')]\n",
    "    \n",
    "    # Make sure all paths are absolute\n",
    "    file_paths = [os.path.abspath(f) for f in file_paths]\n",
    "    \n",
    "    # Filter out non-existent files\n",
    "    valid_files = [f for f in file_paths if os.path.isfile(f)]\n",
    "    invalid_files = set(file_paths) - set(valid_files)\n",
    "    \n",
    "    if invalid_files:\n",
    "        print(f\"Warning: {len(invalid_files)} specified files do not exist and will be skipped:\")\n",
    "        for f in invalid_files:\n",
    "            print(f\"  - {f}\")\n",
    "    \n",
    "    if not valid_files:\n",
    "        print(\"No valid files to process\")\n",
    "        return {\"total\": 0, \"successful\": 0, \"failed\": 0, \"time\": 0}\n",
    "    \n",
    "    print(f\"Found {len(valid_files)} valid files to process\")\n",
    "    \n",
    "    # Create output base directory if specified\n",
    "    if output_base_dir is not None:\n",
    "        os.makedirs(output_base_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each file\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, file_path in enumerate(valid_files, 1):\n",
    "        print(f\"\\n[{i}/{len(valid_files)}] Processing: {file_path}\")\n",
    "        \n",
    "        # Determine the output directory\n",
    "        if output_base_dir is not None:\n",
    "            if preserve_structure:\n",
    "                # Keep partial directory structure\n",
    "                parent_dir = os.path.basename(os.path.dirname(file_path))\n",
    "                file_output_dir = os.path.join(output_base_dir, parent_dir, os.path.splitext(os.path.basename(file_path))[0])\n",
    "            else:\n",
    "                # Flatten the structure\n",
    "                file_output_dir = os.path.join(output_base_dir, os.path.splitext(os.path.basename(file_path))[0])\n",
    "        else:\n",
    "            # Use same directory as the file\n",
    "            file_output_dir = os.path.join(os.path.dirname(file_path), os.path.splitext(os.path.basename(file_path))[0] + \"_csv\")\n",
    "        \n",
    "        if nc_to_csv_xarray(file_path, file_output_dir, engine):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Specific Files Conversion Summary:\")\n",
    "    print(f\"Total files processed: {len(valid_files)}\")\n",
    "    print(f\"Successfully converted: {successful}\")\n",
    "    print(f\"Failed to convert: {failed}\")\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return {\n",
    "        \"total\": len(valid_files),\n",
    "        \"successful\": successful,\n",
    "        \"failed\": failed,\n",
    "        \"time\": elapsed_time\n",
    "    }\n",
    "\n",
    "def process_multiple_inputs(input_paths, file_patterns=None, output_base_dir=None, engine='netcdf4', \n",
    "                          recursive=False, max_depth=None, preserve_structure=True):\n",
    "    \"\"\"\n",
    "    Process multiple directories, subdirectories and specific files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_paths : str or list of str\n",
    "        Comma-separated string or list of paths to process. Can include:\n",
    "        - Directories to scan for matching files\n",
    "        - Specific files to process directly\n",
    "    file_patterns : str or list of str, optional\n",
    "        Comma-separated string or list of file patterns to match (default: \"*.nc\")\n",
    "    output_base_dir : str, optional\n",
    "        Base directory for all outputs. If None, each input dir will have its own 'csv_output'\n",
    "    engine : str, optional\n",
    "        Engine to use for opening the files\n",
    "    recursive : bool, optional\n",
    "        Whether to search for files recursively in subdirectories\n",
    "    max_depth : int, optional\n",
    "        Maximum recursion depth (None for no limit)\n",
    "    preserve_structure : bool, optional\n",
    "        Whether to preserve the directory structure in the output\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with statistics for each directory and file set\n",
    "    \"\"\"\n",
    "    # Process input paths (accept both comma-separated string and list)\n",
    "    if isinstance(input_paths, str):\n",
    "        input_paths = [p.strip() for p in input_paths.split(',')]\n",
    "    \n",
    "    # Process file patterns (accept both comma-separated string and list)\n",
    "    if isinstance(file_patterns, str):\n",
    "        file_patterns = [p.strip() for p in file_patterns.split(',')]\n",
    "    \n",
    "    # Separate directories and files\n",
    "    directories = []\n",
    "    specific_files = []\n",
    "\n",
    "    cpath = os.getcwd()\n",
    "    for path in input_paths:\n",
    "        path = cpath+\"/\"+path\n",
    "        if os.path.isdir(path):\n",
    "            directories.append(path)\n",
    "        elif os.path.isfile(path):\n",
    "            specific_files.append(path)\n",
    "        else:\n",
    "            print(f\"Warning: Path not found or not accessible: {path}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    stats = []\n",
    "    \n",
    "    # Process directories\n",
    "    if directories:\n",
    "        print(f\"Processing {len(directories)} directories with patterns: {file_patterns}\")\n",
    "        \n",
    "        for i, input_dir in enumerate(directories, 1):\n",
    "            print(f\"\\n\\n{'='*60}\")\n",
    "            print(f\"Directory {i}/{len(directories)}: {input_dir}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            if output_base_dir is not None:\n",
    "                # Use subdirectory of the base output dir\n",
    "                dir_name = os.path.basename(os.path.normpath(input_dir))\n",
    "                output_dir = os.path.join(output_base_dir, dir_name)\n",
    "            else:\n",
    "                # Create output directory in each input directory\n",
    "                output_dir = None\n",
    "            \n",
    "            # Process the directory\n",
    "            dir_stats = process_directory(\n",
    "                input_dir=input_dir,\n",
    "                output_dir=output_dir,\n",
    "                engine=engine,\n",
    "                file_patterns=file_patterns,\n",
    "                recursive=recursive,\n",
    "                max_depth=max_depth,\n",
    "                preserve_structure=preserve_structure\n",
    "            )\n",
    "            \n",
    "            # Add directory info to stats\n",
    "            dir_stats['directory'] = input_dir\n",
    "            stats.append(dir_stats)\n",
    "    \n",
    "    # Process specific files if any\n",
    "    if specific_files:\n",
    "        print(f\"\\n\\n{'='*60}\")\n",
    "        print(f\"Processing {len(specific_files)} specific files\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        file_output_dir = output_base_dir if output_base_dir else None\n",
    "        file_stats = process_specific_files(\n",
    "            file_paths=specific_files,\n",
    "            output_base_dir=file_output_dir,\n",
    "            engine=engine,\n",
    "            preserve_structure=preserve_structure\n",
    "        )\n",
    "        \n",
    "        # Add file set info to stats\n",
    "        file_stats['directory'] = \"Specific Files\"\n",
    "        stats.append(file_stats)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    total_time = time.time() - start_time\n",
    "    total_files = sum(s['total'] for s in stats)\n",
    "    total_successful = sum(s['successful'] for s in stats)\n",
    "    total_failed = sum(s['failed'] for s in stats)\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"OVERALL CONVERSION SUMMARY:\")\n",
    "    \n",
    "    if directories:\n",
    "        print(f\"Total directories processed: {len(directories)}\")\n",
    "    if specific_files:\n",
    "        print(f\"Total specific files processed: {len(specific_files)}\")\n",
    "        \n",
    "    print(f\"Total files processed: {total_files}\")\n",
    "    print(f\"Total successfully converted: {total_successful}\")\n",
    "    print(f\"Total failed: {total_failed}\")\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create a DataFrame with statistics\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    if not stats_df.empty:\n",
    "        stats_df['success_rate'] = stats_df['successful'] / stats_df['total'] * 100\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "# Example usage in Jupyter Notebook\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Ask the user for a folder name\n",
    "    folder_name = input(\"Please enter the name of the output folder you want to use: \")\n",
    "\n",
    "    # Check if the folder already exists\n",
    "    if os.path.exists(folder_name):\n",
    "        print(f\"The folder '{folder_name}' already exists.\")\n",
    "    else:\n",
    "        # Create the folder\n",
    "        try:\n",
    "            os.makedirs(folder_name)\n",
    "            print(f\"Folder '{folder_name}' has been created successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating the folder: {e}\")\n",
    "    # Input paths - can include directories and specific files\n",
    "    input_paths = input(\"Please enter the names of the input files and folders: \")\n",
    "    #input_paths = \"/home/jovyan/ncin1, /home/jovyan/ncin2, /home/jovyan/ncin3/USM_NGOFS22025042803SALINITY_t054.nc\"\n",
    "    #input_paths = ncin1, ncin2, ncin3/USM_NGOFS22025042803SALINITY_t054.nc\n",
    "    # File patterns to match (comma-separated string or list)\n",
    "    file_patterns = \"*.nc, *.netcdf, *.NC\"\n",
    "\n",
    "    cpath = os.getcwd()\n",
    "    # Common output directory for all (optional)\n",
    "    # common_output_dir = \"/home/jovyan/ncout1\"\n",
    "    common_output_dir = cpath+\"/\"+folder_name\n",
    "    \n",
    "    # Process all inputs with specified file patterns\n",
    "    stats_df = process_multiple_inputs(\n",
    "        input_paths=input_paths,\n",
    "        file_patterns=file_patterns,\n",
    "        output_base_dir=common_output_dir,  # Set to None to use separate output dirs\n",
    "        engine='scipy',                     # Engine to use\n",
    "        recursive=True,                     # Search in subdirectories\n",
    "        max_depth=4,                        # Maximum folder depth for recursion\n",
    "        preserve_structure=True             # Preserve directory structure in output\n",
    "    )\n",
    "    \n",
    "    # Display the statistics DataFrame\n",
    "    print(stats_df)\n",
    "    \n",
    "    # If running in Jupyter, you can create visualizations\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Create a bar chart of successful vs failed conversions\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        stats_df.plot(\n",
    "            x='directory', \n",
    "            y=['successful', 'failed'], \n",
    "            kind='bar', \n",
    "            stacked=True,\n",
    "            color=['green', 'red']\n",
    "        )\n",
    "        plt.title('NetCDF Conversion Results by Directory')\n",
    "        plt.ylabel('Number of Files')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a pie chart of overall success rate\n",
    "        total_success = stats_df['successful'].sum()\n",
    "        total_fail = stats_df['failed'].sum()\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.pie([total_success, total_fail], \n",
    "                labels=['Successful', 'Failed'],\n",
    "                colors=['green', 'red'],\n",
    "                autopct='%1.1f%%',\n",
    "                startangle=90,\n",
    "                explode=(0.05, 0))\n",
    "        plt.title('Overall NetCDF Conversion Results')\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd911a5-7a1a-474a-9572-c2a6623bacac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
